

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>PsychInsight: A high-level API for creating and epxloring novel face stimuli &#8212; MIND2023 Interacting Minds</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/generative_faces';</script>
    <link rel="canonical" href="http://tutorials.mindsummerschool.com/content/generative_faces.html" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Dynamic Connectivity" href="timecorr.html" />
    <link rel="prev" title="Models of text and language" href="models_of_text_and_language.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/mind_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/mind_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Contributors.html">Contributors</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Download_Data.html">Download Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="Software.html">Software Installation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Background Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_Introduction_to_Programming.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_Introduction_to_Pandas.html">Introduction to Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_Introduction_to_Plotting.html">Introduction to Plotting</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_Introduction_to_Neuroimaging_Data.html">Introduction to Neuroimaging Data</a></li>
<li class="toctree-l1"><a class="reference external" href="http://dartbrains.org/">Dartbrains Neuroimaging Analysis Course</a></li>
<li class="toctree-l1"><a class="reference external" href="http://naturalistic-data.org">Naturalistic Data Analysis Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Analysis Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Functional_Alignment.html">Functional Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="Intersubject_Correlation.html">Synchrony &amp; Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="ANN_tutorial.html">Introduction to programming artificial neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="models_of_text_and_language.html">Models of text and language</a></li>







<li class="toctree-l1 current active"><a class="current reference internal" href="#">PsychInsight: A high-level API for creating and epxloring novel face stimuli</a></li>
<li class="toctree-l1"><a class="reference internal" href="timecorr.html">Dynamic Connectivity</a></li>
<li class="toctree-l1"><a class="reference internal" href="hypertools.html">Visualizing High Dimensional Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contributing Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/mind-tutorials/mind_book">GitHub Repository</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/mind-tutorials/mind_book/master?urlpath=tree/content/generative_faces.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/mind-tutorials/mind_book/blob/master/content/generative_faces.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/mind-tutorials/mind_book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/mind-tutorials/mind_book/edit/master/content/generative_faces.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/mind-tutorials/mind_book/issues/new?title=Issue%20on%20page%20%2Fcontent/generative_faces.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/content/generative_faces.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>PsychInsight: A high-level API for creating and epxloring novel face stimuli</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting started</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-generator">Using the Generator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-api-directly">Using the API directly</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technology-stack">Technology stack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contributions">Contributions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="psychinsight-a-high-level-api-for-creating-and-epxloring-novel-face-stimuli">
<h1>PsychInsight: A high-level API for creating and epxloring novel face stimuli<a class="headerlink" href="#psychinsight-a-high-level-api-for-creating-and-epxloring-novel-face-stimuli" title="Permalink to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Take a look at the image below. What do you see? The contours and colors of the image coalesce into an easily nameable percept: a human face. But your percept is likely not so generic; what can you tell me about the face? When we look at such a photograph, in some sense we can’t help but <em>read</em> (into) it. There are some features that we can realiably read “out” or extract from face photographs, such as age (Henss, 1991; Montepare &amp; Zebrowitz, 1998). However, we also often read “into” faces, as when we form (often misguided) impressions of others’ personalities solely on the basis of a single image (for a review, see Todorov, 2017). We form these sorts of impressions extraordinarily quickly — many perceived attributes reach reliable self-agreement at only 33-50ms stimulus exposures (e.g., Colombatto, Uddenberg, &amp; Scholl, 2021; Todorov et al., 2009) — and early in life, with some facial impression dimensions coming “online” at only 3 years old (Charlesworth et al., 2019). Crucially, such face judgments often show high levels of agreement <em>between</em> perceivers, allowing scientists to create generative models (Gerig et al., 2018; Oosterhof &amp; Todorov, 2008; Peterson, Uddenberg, Griffiths, Todorov, &amp; Suchow, 2022) that can help elucidate not only the features of images driving such impressions, but also the boundary conditions and real-world consequences of such biases (Sutherland et al., 2017). It is important to note that these are not models of reality, but of impression formation — they reflect what (the experimentally sampled) people <em>think</em> an attribute <em>looks like</em> in <em>particular photographs</em> of faces. They act as mirrors that reflect the psychological processes of perceivers, allowing social scientists to transform bias itself into the object of study, or to leverage what we know of how such biases express themselves in order to explore yet subtler effects (Uddenberg &amp; Scholl, 2018). For example, one can use synthetic faces manipulated along perceived attributes to explore how stereotypes propagate, even when there is no evidence for the stereotype at all (Uddenberg, Thompson, Vlasceanu, Griffiths, &amp; Todorov, 2023).</p>
<img src="../images/generative_faces/example_psychinsight_face.jpg" width="400" style="display:block; margin: auto;"/>
<p style="text-align: center">An example of our synthetic face stimuli.</p><p>In order to study how we judge, perceive, or remember faces, we must use face stimuli. Broadly speaking, these stimuli can either be “real” (e.g., photographs of real-world individuals) or “synthetic” (e.g., computer-generated images, drawings, etc.). Both approaches come with tradeoffs. On the one hand, real stimuli are ecologically valid and entirely convincing to participants, but they often lack in diversity (e.g., representing different racialized groups or a wide range of emotional expressions) and ease of experimental control (e.g., requiring the manual use of photo editing software to transform the stimuli in the precise ways one might need). In addition, there are real privacy concerns with using such real-life stimuli — imagine having <em>your</em> photo shown to thousands of strangers to be rated on all sorts of dimensions, perhaps on a job or dating website! — which necessitates restrictions on how they are used in experiments, and sometimes, even how they are presented in papers and conferences (Ma et al., 2015). Lastly, as more and more labs utilize online experiment participant pools, widespread use of face datasets may become (or may already be) a problem, with fewer and fewer (stimulus-)naïve participants available for testing.</p>
<p>On the other hand, computer-generated impression models have a few key advantages, including the elimination (or, at the very least, <em>extreme</em> mitigation) of privacy concerns in IRB-approved research, as well as greater ease of experimental control. However, there are two salient and common disadvantages: (1) lack of realism — the most popular computer-generated images in use are very obviously computer-generated (Oosterhof &amp; Todorov, 2008) — and (2) lack of accessibility, often being limited to the research group that created them via explicit access controls or implicit technical and knowledge barriers. Last year, we published a paper that, among other things, attempted to mitigate the problem of realism (Peterson et al., 2022). This chapter marks our first step toward solving the latter issue: here, we provide a high level API for the generation, transformation, and inference of face stimuli along dozens of perceived attributes of psychological interest. The face above is one example of the kind of stimuli that you will be able to create by the end of this chapter. This chapter will not cover the details of how these models were made, as they have already been documented in detail. Instead, you may refer to our work in PNAS (Peterson et al., 2022) and NVIDIA’s CVPR paper (Karras et al., 2020) if you are interested in understanding the experimental and computational approaches taken.</p>
<div class="alert alert-block alert-info">
    Note that work on this tool is still ongoing, so if you encounter any errors or other issues, please contact me directly at <a href="mailto:stefan.uddenberg@chicagobooth.edu">stefan.uddenberg@chicagobooth.edu</a>. To limit abuse of our computational resources, the precise URL(s) used for the application will be provided at the summer school. It is our hope to open the app up for broader academic use after taking several rounds feedback from you and other researchers, so please let us know what you think!
</div></section>
<section id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this heading">#</a></h2>
<p>To start, all you’ll need is a web browser. We’ll be making an account and exploring some basic features of the API from a simple frontend.</p>
<ul class="simple">
<li><p>First, head to the application website and click on the “Get started” button or the “Register” button within the navigation bar toward the top right of the page.</p>
<ul>
<li><p>Fill out the registration form (including the project description section) and click “Submit”.</p></li>
<li><p>Check your email for an account verification link and click on it.</p></li>
<li><p>After you’ve been verified, you should be able to use your credentials to login.</p></li>
</ul>
</li>
<li><p>Once you have an account, login using the “Login” button toward the top right of the page.</p></li>
<li><p>After logging in:</p>
<ul>
<li><p>You will automatically be directed to the “Generator” app (described in more detail below).</p></li>
<li><p>At the home page, also accessible via the “API Token” page, you will be able to copy your access token by pressing the relevant button onscreen. Your access token is a long string of characters that you can use as a credential to access the API from either the frontend or your chosen means of consuming APIs in general. It expires after a set period of time.</p></li>
<li><p>You can view your profile details at the “Profile” page. The ability to edit (at least some of) those details will come in the near future.</p></li>
<li><p>Once your email is verified and your account is approved, you should be able to see and click on the “Generator” button, once again, toward the top right.</p></li>
</ul>
</li>
</ul>
<img src="../images/generative_faces/example_main_page.png" width="800" style="display: block; margin: auto;"/>
<p style="text-align: center">The landing page of the frontend.</p>        </section>
<section id="using-the-generator">
<h2>Using the Generator<a class="headerlink" href="#using-the-generator" title="Permalink to this heading">#</a></h2>
<p>The “Generator” app is a simple face stimulus generator and explorer that showcases some of the features of the API. The more advanced features can be used by directly interfacing with the API without the mediation of the frontend (more on that below).</p>
<ul class="simple">
<li><p>Click the “Generate” button to create a new face.</p></li>
<li><p>After the image appears, the sidebar will guide you through the next steps. First, a search box of perceived attributes will appear. Upon selecting a perceived attribute, you can use the slider or the input box above to select your desired (predicted) level of that perceived attribute for the new face. You can also choose up to 2 attributes to hold constant in the process using the input box below. Click “Transform” when you would like to see the result, and the image should be updated in short order.</p>
<ul>
<li><p>In the frontend application, values are limited to between -4 and +4 standard deviations from the mean predicted judgment (centered at 0). The images are more susceptible to artifacts and potential misuse past these values.</p></li>
<li><p>The math works such that you can control for any number of attributes. However, in practice, the images don’t <em>look</em> controlled. This becomes more obvious the more perceived attributes you attempt to hold constant.</p></li>
</ul>
</li>
<li><p>You can toggle the judgment predictions with the “Show predictions” toggle. They are shown below the image in table format, and the modification sliders are automatically set to those values.</p></li>
</ul>
<img src="../images/generative_faces/example_generator_image.png" width="800" style="display: block; margin: auto;"/>
<p style="text-align: center">The Generator application at work.</p><div class="alert alert-block alert-info">
    Note well: these perceived attribute models reflect patterns in the impressions of a very large online sample of Americans collected in the early 2020s. They were designed to study how people see, remember, and judge others — they are not meant to reify any such characteristics. Indeed, they could not in principle, because there are no ground truth measurements in the data. Any attempt to do so with the current impression models would be both ethically misguided and doomed to failure.
</div></section>
<section id="using-the-api-directly">
<h2>Using the API directly<a class="headerlink" href="#using-the-api-directly" title="Permalink to this heading">#</a></h2>
<p>The API is agnostic to what program (e.g., Insomnia, Postman, curl) or programming language (e.g., Python, R, JavaScript) you use to make requests. It simply requires that requests be made to certain endpoints in certain formats. The documentation can be seen at <code class="docutils literal notranslate"><span class="pre">{redacted_api_url}/docs</span></code> along with a simple interface for testing out these different endpoints. Many of them require a valid token (which you can copy from the main application) from an approved account — you can click the green “Authorize” button at the top right of the screen in order to use those endpoints, or include the token in your authorization header if making requests on your own.</p>
<p>Direct use of the API allows for the generation, transformation, and prediction of many, many faces at once, whereas the “Generator” frontend application only allows you to explore one face at a time — we’re still working out what the appropriate UX should be when using a web browser for such requests, and your feedback and suggestions will surely come in handy!</p>
</section>
<section id="technology-stack">
<h2>Technology stack<a class="headerlink" href="#technology-stack" title="Permalink to this heading">#</a></h2>
<p>The API is made with FastAPI on top of PostgreSQL, deployed on Railway. The frontend is made with Sveltekit and the Skeleton UI framework, deployed on Vercel. All face operations are performed by an AWS EC2 instance running custom code on top of StyleGAN2.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<p>Charlesworth, T. E. S., Hudson, S. T. J., Cogsdill, E. J., Spelke, E. S., &amp; Banaji, M. R. (2019). Children use targets’ facial appearance to guide and predict social behavior. <em>Developmental Psychology</em>, <em>55</em>(7), 1400–1413.</p>
<p>Colombatto, C., <strong>Uddenberg, S.</strong>, &amp; Scholl, B. J. (2021). The efficiency of demography in face perception. <em>Attention, Perception, &amp; Psychophysics</em>, <em>83</em>(8), 3104–3117.</p>
<p>Gerig, T., Morel-Forster, A., Blumer, C., Egger, B., Luthi, M., Schönborn, S., &amp; Vetter, T. (2018, May). Morphable face models-an open framework. In <em>2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018)</em> (pp. 75-82). IEEE.</p>
<p>Henss, R. (1991). Perceiving age and attractiveness in facial photographs. <em>Journal of Applied Social Psychology</em>, <em>21</em>(11), 933–946.</p>
<p>Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., &amp; Aila, T. (2020). Analyzing and improving the image quality of StyleGAN. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> (pp. 8107–8116).</p>
<p>Ma, D. S., Correll, J., &amp; Wittenbrink, B. (2015). The Chicago face database: A free stimulus set of faces and norming data. <em>Behavior Research Methods</em>, <em>47</em>(4), 1122–1135.</p>
<p>Montepare, J. M., &amp; Zebrowitz, L. A. (1998). Person perception comes of age: The salience and significance of age in social judgments. In M. P. Zanna (Ed.), <em>Advances in Experimental Social Psychology</em> (Vol. 30, pp. 93–161). Academic Press.</p>
<p>Oosterhof, N. N., &amp; Todorov, A. (2008). The functional basis of face evaluation. <em>Proceedings of the National Academy of Sciences</em>, <em>105</em>(32), 11087–11092.</p>
<p>Peterson, J. C., <strong>Uddenberg, S.</strong>, Griffiths, T. L., Todorov, A., &amp; Suchow, J. W. (2022). Deep models of superficial face judgments. <em>Proceedings of the National Academy of Sciences</em>, <em>119</em>(17), e2115228119.</p>
<p>Sutherland, C. A. M., Rhodes, G., &amp; Young, A. W. (2017). Facial image manipulation: A tool for investigating social perception. <em>Social Psychological and Personality Science</em>, <em>8</em>(5), 538–551.</p>
<p>Todorov, A. (2017). <em>Face value: The irresistible influence of first impressions.</em> Princeton University Press. <a class="reference external" href="https://doi.org/10.1515/9781400885725">https://doi.org/10.1515/9781400885725</a></p>
<p>Todorov, A., Pakrashi, M., &amp; Oosterhof, N. N. (2009). Evaluating faces on trustworthiness after minimal time exposure. Social Cognition, 27(6), 813–833. <a class="reference external" href="https://doi.org/10.1521/soco.2009.27.6.813">https://doi.org/10.1521/soco.2009.27.6.813</a></p>
<p><strong>Uddenberg, S.</strong>, &amp; Scholl, B. J. (2018). Teleface: Serial reproduction of faces reveals a whiteward bias in race memory. <em>Journal of Experimental Psychology: General</em>, <em>147</em>(10), 1466–1487.</p>
<p><strong>Uddenberg, S.</strong>, Thompson, B., Vlasceanu, M., Griffiths, T. L., &amp; Todorov, A. (2023). Iterated learning reveals stereotypes of facial trustworthiness that propagate in the absence of evidence. <em>Cognition</em>, <em>237</em>, 105452.</p>
</section>
<section id="contributions">
<h2>Contributions<a class="headerlink" href="#contributions" title="Permalink to this heading">#</a></h2>
<p>This notebook was authored by Stefan Uddenberg. Code for the API and/or models was contributed by Stefan Uddenberg, Rachit Shah, and Daniel Albohn. Special thanks to Joshua Peterson and Jordan Suchow for their pioneering work on the first version of the underlying face modeling code, circa 2021, and to NVIDIA for releasing the open source models and datasets used to underpin our current work.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "pytorch-kai"
        },
        kernelOptions: {
            name: "pytorch-kai",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'pytorch-kai'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="models_of_text_and_language.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Models of text and language</p>
      </div>
    </a>
    <a class="right-next"
       href="timecorr.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Dynamic Connectivity</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting started</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-generator">Using the Generator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-api-directly">Using the API directly</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#technology-stack">Technology stack</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contributions">Contributions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Luke Chang
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  Created by <a href="http://www.lukejchang.com/">Luke Chang</a> using <a href="https://jupyterbook.org/">Jupyter Book</a> and supported by NSF (CAREER Award 1848370) and the <a href="https://www.interactingminds.com/">Consortium for Interacting Minds</a>. Please post any questions on our <a href="https://www.askpbs.org/c/mind-summer-school/">Discourse Page</a>.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>