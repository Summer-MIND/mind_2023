{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to autoencoders for psychologists\n",
        "#### Tutorial for Methods In Neuroscience at Dartmouth ([MIND](http://mindsummerschool.org/)) 2023\n",
        "By [Mark A. Thornton](http://markallenthornton.com/)\n",
        "\n",
        "This tutorial provides a guide to thinking about and programming autoencoders, a type of deep neural network architecture. This tutorial is designed to follow my basic introduction to programming deep nets in pytorch. If you haven't already seen that tutorial, you can find it [here](https://colab.research.google.com/drive/1VwGHNRitvehEqCe47M9J4dAQSF5KnYbg?usp=sharing).\n",
        "\n",
        "Autoencoders are a specialized class of deep neural networks that learn compressed representations of data. To accomplish this compression, autoencoders use a characteristic \"hourglass\" shape illustrated below.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png)\n",
        "\n",
        "The hourglass consists of three parts:\n",
        "* An encoder, which compresses the input data through layers with smaller and smaller numbers of units.\n",
        "* A bottleneck layer, at the center of the network, which contains the lowest dimensional representation, or \"code\" for the input data.\n",
        "* A decoder, which learns to expand the highly compressed code in the bottleneck back out to produce an output that reconstructs the model's inputs.\n",
        "\n",
        "Here's we will learn about autoencoders in the context of compressing survey results, using PCA as a more familiar comparison case.\n"
      ],
      "metadata": {
        "id": "0o_BU_DqHtsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "kEIn4-ZhLre6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import packages"
      ],
      "metadata": {
        "id": "tjtbAJyyLvQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHzRvYYVPDJm"
      },
      "outputs": [],
      "source": [
        "import urllib, os, zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.stats import zscore\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect and set hardware device\n",
        "Depending on where you run your notebook, you may be able to take advantage of different hardware. If a cuda-enable graphics card is available, this will be preferred. Mac chipsets (MPS) and traditional processors (CPU) are the fallback options. On Colab, you may want to change your runtime type to take advantage of the GPU runtimes they offer."
      ],
      "metadata": {
        "id": "HP_qxVWikiK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df-s6p7lkddN",
        "outputId": "9356cfe7-47df-41ec-e42e-36c56ba81613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the data\n",
        "Check if data has been downloaded; if not, do so and unzip it.\n",
        "\n",
        "These data come from a 50-item Big Five survey collected by the Open-Source Psychometrics Project. They can be retrieved from https://openpsychometrics.org/_rawdata/ but I have rehosted them for this tutorial to avoid adding load to their server."
      ],
      "metadata": {
        "id": "Cc0wt-QYLx5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfiles = os.listdir(\"./sample_data/\")\n",
        "fname = 'ipip_big5.csv'\n",
        "if not fname in dfiles:\n",
        "  datafile = urllib.request.URLopener()\n",
        "  datafile.retrieve(\"http://markallenthornton.com/data/ipip_big5.zip\", \"./sample_data/data.zip\")\n",
        "  with zipfile.ZipFile(\"./sample_data/data.zip\") as zip_ref:\n",
        "    zip_ref.extractall(\"./sample_data/\")"
      ],
      "metadata": {
        "id": "_UOSSLyzVQGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data into Python, filter out unnecessary items and observations with missing values, then transform all values to the [0,1] interval."
      ],
      "metadata": {
        "id": "iNy9tmqOXwcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dat = pd.read_csv('./sample_data/ipip_big5.csv', sep='\\t')\n",
        "dat = dat.iloc[:,:50]  # filters out variables except items\n",
        "dat = dat.loc[dat.min(1)!=0,]  # filters out missing values (0s)\n",
        "dat = (dat-1)/4 # transform to 0-1 scale\n",
        "dat.dropna(inplace=True)\n",
        "dat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM02oLTZWtyt",
        "outputId": "a4efabba-243a-432e-decb-926b9af7138a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(874434, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reverse score the appropriate values."
      ],
      "metadata": {
        "id": "3wzLVqdOMs6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "revd = [2, 4, 6, 8, 10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 23, 25, 27, 32, 34, 36, 38, 42, 44, 46]\n",
        "revd = [r-1 for r in revd]\n",
        "for r in revd:\n",
        "    dat.iloc[:,r] = 1-dat.iloc[:,r]"
      ],
      "metadata": {
        "id": "kacikscgYODi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the full dataset into training (90%), validation (5%), and test (5%) sets and scale to mean = 0, SD = 1 (based on training data parameters)."
      ],
      "metadata": {
        "id": "Ws6LwxTiM17f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split data\n",
        "train = dat.sample(frac=.9,random_state=0)\n",
        "testval = dat.drop(train.index)\n",
        "test = testval.sample(frac=.5,random_state=0)\n",
        "val = testval.drop(test.index)\n",
        "\n",
        "# scale data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train)\n",
        "train = pd.DataFrame(scaler.transform(train))\n",
        "test = pd.DataFrame(scaler.transform(test))\n",
        "val = pd.DataFrame(scaler.transform(val))\n",
        "\n",
        "print((train.shape[0],test.shape[0],val.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDMnEcSCYr0B",
        "outputId": "27e4925c-e394-4b9a-81ff-351bd90593d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(786991, 43722, 43721)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score the survey in the way that was intended (i.e., by taking the means of the subscales)"
      ],
      "metadata": {
        "id": "nc6gSHi6Nhdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# score 'appropriately' (mean of items)\n",
        "trainScored = pd.concat([train.iloc[:,:10].mean(1),\n",
        "             train.iloc[:,10:20].mean(1),\n",
        "             train.iloc[:,20:30].mean(1),\n",
        "             train.iloc[:,30:40].mean(1),\n",
        "             train.iloc[:,40:50].mean(1)],axis=1)\n",
        "\n",
        "testScored = pd.concat([test.iloc[:,:10].mean(1),\n",
        "             test.iloc[:,10:20].mean(1),\n",
        "             test.iloc[:,20:30].mean(1),\n",
        "             test.iloc[:,30:40].mean(1),\n",
        "             test.iloc[:,40:50].mean(1)],axis=1)"
      ],
      "metadata": {
        "id": "Xhp0vl1CYesf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "Use the means of each subscale in the training set to predict the individual question ratings in the test set. This provides baseline metrics (root mean square error [RMSE] and R<sup>2</sup>) against which to compare our later models."
      ],
      "metadata": {
        "id": "ZIBxZux1Nsov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the mean factor scores\n",
        "ffmeans = trainScored.mean(axis=0)\n",
        "\n",
        "# make predictions about the test set\n",
        "ffmeanpred = np.repeat(np.ndarray((1,50),buffer=np.array(np.repeat(ffmeans,10))),test.shape[0],axis=0)\n",
        "\n",
        "# compute metrics\n",
        "rmse = np.sqrt(mean_squared_error(test,ffmeanpred))\n",
        "\n",
        "# R2\n",
        "r2 = r2_score(test,ffmeanpred)\n",
        "\n",
        "print(\"RMSE = \" + str(rmse) + \", R2 = \" + str(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2lm0z_mYx9m",
        "outputId": "102bed99-8a71-4016-c270-66b210a7a0f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE = 0.9980879091702006, R2 = -2.0687293782364335e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the standardization of the data, an RMSE around 1, and R2 around 0 is what we would expect for this baseline."
      ],
      "metadata": {
        "id": "FOzPiyMil1kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scoring\n",
        "Now let's score each participant's data individually (i.e., the way that we would actually score their Big 5 questionnaire, giving equal weight to each question within a subscale)."
      ],
      "metadata": {
        "id": "w5hSvFM1mQFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# individual scoring performance\n",
        "ffind = np.repeat(np.array(testScored),10,1)\n",
        "\n",
        "# compute metrics\n",
        "rmse = np.sqrt(mean_squared_error(test,ffind))\n",
        "\n",
        "# R2\n",
        "r2 = r2_score(test,ffind)\n",
        "\n",
        "print(\"RMSE = \" + str(rmse) + \", R2 = \" + str(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t5p4QrilNS5",
        "outputId": "cc6de81a-32b0-4d73-935f-be48ee0d40f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE = 0.7551913272726057, R2 = 0.42766950509958934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we might hope, scoring the Big 5 properly does a much better job than assuming the scale mean for each item. The RMSE decreases to about 3/4 of baseline error, and variance explained increases to over 40%. However, the scoring procedure assumes that each item in a subscale should be equally weighted. This works reasonably well here because that is how the scale was constructed to operate, but can we do better by dropping this assumption and using PCA instead?"
      ],
      "metadata": {
        "id": "MDjyrg2smhFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA\n",
        "Here we perform a principal component analysis with a five-factor solution."
      ],
      "metadata": {
        "id": "JcWSF4Cnm96t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform PCA on training data\n",
        "trainpca = PCA(n_components=5,)\n",
        "trainpca.fit(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "uwHT7eO9aCqJ",
        "outputId": "35fad4a3-db82-45ba-c89f-b5593aafe3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=5)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=5)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we apply this PCA to predict item-level ratings in the held-out test set."
      ],
      "metadata": {
        "id": "9rUvPmUwnJZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply PCA to test data and evaluate fit\n",
        "testscores = trainpca.transform(test)\n",
        "ffpca = trainpca.inverse_transform(testscores)\n",
        "\n",
        "# compute metrics\n",
        "rmse = np.sqrt(mean_squared_error(test,ffpca))\n",
        "\n",
        "# R2\n",
        "r2 = r2_score(test,ffpca)\n",
        "\n",
        "print(\"RMSE = \" + str(rmse) + \", R2 = \" + str(r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWBGVohtaKtO",
        "outputId": "b80bc9be-a1d4-4cfa-8071-f7f21b01b0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE = 0.731608564621871, R2 = 0.46284261823824274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PCA does better than the scoring method in terms of both of our metrics of interest, but the difference in slight.\n",
        "\n",
        "PCA performs a linear compression of the survey data. To see whether a nonlinear compression could perform better, we will now turn to an autoencoder"
      ],
      "metadata": {
        "id": "dWPWI4psnYSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder\n",
        "In the cell below, we define the structure of our autoencoder. It consists of ReLU layers (with the exception of the bottleneck and output layers) that decrease from 128 units to 8 units in powers of two in the decoder, followed by a 5 unit bottleneck, and then reexpansion out to 128 units before reconstructing the length 50 input as the output. Batch normalization is applied after each ReLU layer."
      ],
      "metadata": {
        "id": "tMJPtS7-nwpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model architecture\n",
        "class autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Linear(50,128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128,64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(64,32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Linear(32,16),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16,8),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.Linear(8,5),\n",
        "            nn.Linear(5,8),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.Linear(8,16),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.Linear(16,32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Linear(32,64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(64,128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Linear(128,50)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        pred = self.layer_stack(x)\n",
        "        return pred\n",
        "\n",
        "ae = autoencoder().to(device)"
      ],
      "metadata": {
        "id": "jGKak4YUasLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class defined the cell below allows us to perform early stopping to prevent overfitting of the autoencoder."
      ],
      "metadata": {
        "id": "DmweX_nPoVCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience=1, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "Wrtk0DomdGTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we set the batch size, turn our dataframes into tensors, then tensor datasets, and finally put them into dataloaders."
      ],
      "metadata": {
        "id": "c3IedI5HoqDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "\n",
        "train_tensor = torch.from_numpy(np.float32(train))\n",
        "train_dataset = TensorDataset(train_tensor, train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_tensor = torch.from_numpy(np.float32(test))\n",
        "test_dataset = TensorDataset(test_tensor, test_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_tensor = torch.from_numpy(np.float32(val))\n",
        "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "liQLa94ldXYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below defines our training procedure for a single epoch."
      ],
      "metadata": {
        "id": "vNgsfxx1ozbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # note that we're actually using the GPU here\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "TnFy0-wcfFd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below allows us to test the performance of our model on a new dataloader."
      ],
      "metadata": {
        "id": "YoXEzOHNo7wU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define test function\n",
        "def test_nn(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, r2 = 0, 0 # evaluate loss (MSE, and R^2)\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            r2 += 1 - torch.sum((y-pred)**2) / torch.sum(y **2)\n",
        "    test_loss /= num_batches\n",
        "    r2 /= num_batches\n",
        "    print(f\"Performance: \\n R^2: {r2:>8f}, RMSE: {np.sqrt(test_loss):>8f} \\n\")\n",
        "    return r2.cpu().numpy(), test_loss"
      ],
      "metadata": {
        "id": "uUYiJPXSfLAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll train the autoencoder."
      ],
      "metadata": {
        "id": "orHf-ttTpAZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seeds\n",
        "torch.manual_seed(0) # pytorch's random seed\n",
        "np.random.seed(0) # numpy's random seed\n",
        "early_stopper = EarlyStopper(patience=1, min_delta=0)\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(ae.parameters(),lr=.001)\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_nn(train_dataloader, ae, loss, optimizer)\n",
        "    val_loss = test_nn(val_dataloader, ae, loss)[1]\n",
        "    if early_stopper.early_stop(val_loss):\n",
        "        break\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O--DK8mLdN5E",
        "outputId": "8a8c48a9-95a4-47ce-bfea-db3ed1b49ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.381381  [  128/786991]\n",
            "loss: 0.808010  [12928/786991]\n",
            "loss: 0.787175  [25728/786991]\n",
            "loss: 0.651829  [38528/786991]\n",
            "loss: 0.699782  [51328/786991]\n",
            "loss: 0.641226  [64128/786991]\n",
            "loss: 0.548250  [76928/786991]\n",
            "loss: 0.574619  [89728/786991]\n",
            "loss: 0.592357  [102528/786991]\n",
            "loss: 0.607343  [115328/786991]\n",
            "loss: 0.594988  [128128/786991]\n",
            "loss: 0.600100  [140928/786991]\n",
            "loss: 0.566155  [153728/786991]\n",
            "loss: 0.614944  [166528/786991]\n",
            "loss: 0.614284  [179328/786991]\n",
            "loss: 0.569217  [192128/786991]\n",
            "loss: 0.598552  [204928/786991]\n",
            "loss: 0.629417  [217728/786991]\n",
            "loss: 0.584816  [230528/786991]\n",
            "loss: 0.568159  [243328/786991]\n",
            "loss: 0.572322  [256128/786991]\n",
            "loss: 0.602150  [268928/786991]\n",
            "loss: 0.580945  [281728/786991]\n",
            "loss: 0.589399  [294528/786991]\n",
            "loss: 0.583255  [307328/786991]\n",
            "loss: 0.595573  [320128/786991]\n",
            "loss: 0.561147  [332928/786991]\n",
            "loss: 0.607054  [345728/786991]\n",
            "loss: 0.588312  [358528/786991]\n",
            "loss: 0.606857  [371328/786991]\n",
            "loss: 0.606584  [384128/786991]\n",
            "loss: 0.566202  [396928/786991]\n",
            "loss: 0.564343  [409728/786991]\n",
            "loss: 0.608810  [422528/786991]\n",
            "loss: 0.623708  [435328/786991]\n",
            "loss: 0.546184  [448128/786991]\n",
            "loss: 0.587713  [460928/786991]\n",
            "loss: 0.531495  [473728/786991]\n",
            "loss: 0.579431  [486528/786991]\n",
            "loss: 0.574155  [499328/786991]\n",
            "loss: 0.527481  [512128/786991]\n",
            "loss: 0.560739  [524928/786991]\n",
            "loss: 0.542594  [537728/786991]\n",
            "loss: 0.563898  [550528/786991]\n",
            "loss: 0.546413  [563328/786991]\n",
            "loss: 0.550738  [576128/786991]\n",
            "loss: 0.595592  [588928/786991]\n",
            "loss: 0.600887  [601728/786991]\n",
            "loss: 0.577264  [614528/786991]\n",
            "loss: 0.539533  [627328/786991]\n",
            "loss: 0.588137  [640128/786991]\n",
            "loss: 0.552017  [652928/786991]\n",
            "loss: 0.574613  [665728/786991]\n",
            "loss: 0.563369  [678528/786991]\n",
            "loss: 0.524398  [691328/786991]\n",
            "loss: 0.530872  [704128/786991]\n",
            "loss: 0.546786  [716928/786991]\n",
            "loss: 0.543316  [729728/786991]\n",
            "loss: 0.549501  [742528/786991]\n",
            "loss: 0.538662  [755328/786991]\n",
            "loss: 0.547396  [768128/786991]\n",
            "loss: 0.540128  [780928/786991]\n",
            "Performance: \n",
            " R^2: 0.445710, RMSE: 0.743082 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.581356  [  128/786991]\n",
            "loss: 0.576343  [12928/786991]\n",
            "loss: 0.538165  [25728/786991]\n",
            "loss: 0.569860  [38528/786991]\n",
            "loss: 0.542284  [51328/786991]\n",
            "loss: 0.540319  [64128/786991]\n",
            "loss: 0.542121  [76928/786991]\n",
            "loss: 0.567295  [89728/786991]\n",
            "loss: 0.572850  [102528/786991]\n",
            "loss: 0.548670  [115328/786991]\n",
            "loss: 0.518049  [128128/786991]\n",
            "loss: 0.510620  [140928/786991]\n",
            "loss: 0.510832  [153728/786991]\n",
            "loss: 0.578314  [166528/786991]\n",
            "loss: 0.550837  [179328/786991]\n",
            "loss: 0.531730  [192128/786991]\n",
            "loss: 0.534473  [204928/786991]\n",
            "loss: 0.527316  [217728/786991]\n",
            "loss: 0.505620  [230528/786991]\n",
            "loss: 0.523841  [243328/786991]\n",
            "loss: 0.537340  [256128/786991]\n",
            "loss: 0.484429  [268928/786991]\n",
            "loss: 0.475819  [281728/786991]\n",
            "loss: 0.559521  [294528/786991]\n",
            "loss: 0.568359  [307328/786991]\n",
            "loss: 0.520667  [320128/786991]\n",
            "loss: 0.475745  [332928/786991]\n",
            "loss: 0.481422  [345728/786991]\n",
            "loss: 0.541088  [358528/786991]\n",
            "loss: 0.521733  [371328/786991]\n",
            "loss: 0.539423  [384128/786991]\n",
            "loss: 0.558421  [396928/786991]\n",
            "loss: 0.516178  [409728/786991]\n",
            "loss: 0.545732  [422528/786991]\n",
            "loss: 0.510520  [435328/786991]\n",
            "loss: 0.530255  [448128/786991]\n",
            "loss: 0.522389  [460928/786991]\n",
            "loss: 0.544091  [473728/786991]\n",
            "loss: 0.507075  [486528/786991]\n",
            "loss: 0.528195  [499328/786991]\n",
            "loss: 0.549312  [512128/786991]\n",
            "loss: 0.548037  [524928/786991]\n",
            "loss: 0.524154  [537728/786991]\n",
            "loss: 0.557498  [550528/786991]\n",
            "loss: 0.530746  [563328/786991]\n",
            "loss: 0.550896  [576128/786991]\n",
            "loss: 0.506411  [588928/786991]\n",
            "loss: 0.525843  [601728/786991]\n",
            "loss: 0.533281  [614528/786991]\n",
            "loss: 0.554565  [627328/786991]\n",
            "loss: 0.512287  [640128/786991]\n",
            "loss: 0.527376  [652928/786991]\n",
            "loss: 0.506266  [665728/786991]\n",
            "loss: 0.565507  [678528/786991]\n",
            "loss: 0.523003  [691328/786991]\n",
            "loss: 0.501312  [704128/786991]\n",
            "loss: 0.555041  [716928/786991]\n",
            "loss: 0.500508  [729728/786991]\n",
            "loss: 0.526605  [742528/786991]\n",
            "loss: 0.537569  [755328/786991]\n",
            "loss: 0.482678  [768128/786991]\n",
            "loss: 0.512443  [780928/786991]\n",
            "Performance: \n",
            " R^2: 0.488428, RMSE: 0.713865 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.494248  [  128/786991]\n",
            "loss: 0.538949  [12928/786991]\n",
            "loss: 0.533517  [25728/786991]\n",
            "loss: 0.556363  [38528/786991]\n",
            "loss: 0.485362  [51328/786991]\n",
            "loss: 0.508803  [64128/786991]\n",
            "loss: 0.555400  [76928/786991]\n",
            "loss: 0.523322  [89728/786991]\n",
            "loss: 0.522090  [102528/786991]\n",
            "loss: 0.569530  [115328/786991]\n",
            "loss: 0.538021  [128128/786991]\n",
            "loss: 0.513764  [140928/786991]\n",
            "loss: 0.525888  [153728/786991]\n",
            "loss: 0.497379  [166528/786991]\n",
            "loss: 0.522018  [179328/786991]\n",
            "loss: 0.529834  [192128/786991]\n",
            "loss: 0.502123  [204928/786991]\n",
            "loss: 0.519386  [217728/786991]\n",
            "loss: 0.532086  [230528/786991]\n",
            "loss: 0.494880  [243328/786991]\n",
            "loss: 0.492824  [256128/786991]\n",
            "loss: 0.501412  [268928/786991]\n",
            "loss: 0.502325  [281728/786991]\n",
            "loss: 0.536450  [294528/786991]\n",
            "loss: 0.475880  [307328/786991]\n",
            "loss: 0.514011  [320128/786991]\n",
            "loss: 0.517949  [332928/786991]\n",
            "loss: 0.532189  [345728/786991]\n",
            "loss: 0.476256  [358528/786991]\n",
            "loss: 0.544477  [371328/786991]\n",
            "loss: 0.513465  [384128/786991]\n",
            "loss: 0.510476  [396928/786991]\n",
            "loss: 0.512786  [409728/786991]\n",
            "loss: 0.505997  [422528/786991]\n",
            "loss: 0.550501  [435328/786991]\n",
            "loss: 0.491287  [448128/786991]\n",
            "loss: 0.514817  [460928/786991]\n",
            "loss: 0.476437  [473728/786991]\n",
            "loss: 0.514649  [486528/786991]\n",
            "loss: 0.502649  [499328/786991]\n",
            "loss: 0.504462  [512128/786991]\n",
            "loss: 0.495073  [524928/786991]\n",
            "loss: 0.502039  [537728/786991]\n",
            "loss: 0.528314  [550528/786991]\n",
            "loss: 0.498849  [563328/786991]\n",
            "loss: 0.503437  [576128/786991]\n",
            "loss: 0.503335  [588928/786991]\n",
            "loss: 0.506433  [601728/786991]\n",
            "loss: 0.518092  [614528/786991]\n",
            "loss: 0.506114  [627328/786991]\n",
            "loss: 0.569227  [640128/786991]\n",
            "loss: 0.524341  [652928/786991]\n",
            "loss: 0.552468  [665728/786991]\n",
            "loss: 0.544550  [678528/786991]\n",
            "loss: 0.487729  [691328/786991]\n",
            "loss: 0.482548  [704128/786991]\n",
            "loss: 0.518818  [716928/786991]\n",
            "loss: 0.481246  [729728/786991]\n",
            "loss: 0.523766  [742528/786991]\n",
            "loss: 0.514103  [755328/786991]\n",
            "loss: 0.520792  [768128/786991]\n",
            "loss: 0.498576  [780928/786991]\n",
            "Performance: \n",
            " R^2: 0.494445, RMSE: 0.709651 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.504738  [  128/786991]\n",
            "loss: 0.508473  [12928/786991]\n",
            "loss: 0.523681  [25728/786991]\n",
            "loss: 0.508766  [38528/786991]\n",
            "loss: 0.525337  [51328/786991]\n",
            "loss: 0.505255  [64128/786991]\n",
            "loss: 0.513530  [76928/786991]\n",
            "loss: 0.508912  [89728/786991]\n",
            "loss: 0.509716  [102528/786991]\n",
            "loss: 0.482944  [115328/786991]\n",
            "loss: 0.520289  [128128/786991]\n",
            "loss: 0.506963  [140928/786991]\n",
            "loss: 0.525367  [153728/786991]\n",
            "loss: 0.493516  [166528/786991]\n",
            "loss: 0.522452  [179328/786991]\n",
            "loss: 0.480405  [192128/786991]\n",
            "loss: 0.515450  [204928/786991]\n",
            "loss: 0.524694  [217728/786991]\n",
            "loss: 0.547909  [230528/786991]\n",
            "loss: 0.497184  [243328/786991]\n",
            "loss: 0.490725  [256128/786991]\n",
            "loss: 0.497367  [268928/786991]\n",
            "loss: 0.517799  [281728/786991]\n",
            "loss: 0.533765  [294528/786991]\n",
            "loss: 0.504540  [307328/786991]\n",
            "loss: 0.472663  [320128/786991]\n",
            "loss: 0.545336  [332928/786991]\n",
            "loss: 0.537826  [345728/786991]\n",
            "loss: 0.542009  [358528/786991]\n",
            "loss: 0.521217  [371328/786991]\n",
            "loss: 0.511429  [384128/786991]\n",
            "loss: 0.533820  [396928/786991]\n",
            "loss: 0.502363  [409728/786991]\n",
            "loss: 0.518243  [422528/786991]\n",
            "loss: 0.520844  [435328/786991]\n",
            "loss: 0.518942  [448128/786991]\n",
            "loss: 0.461996  [460928/786991]\n",
            "loss: 0.515288  [473728/786991]\n",
            "loss: 0.519074  [486528/786991]\n",
            "loss: 0.512588  [499328/786991]\n",
            "loss: 0.487156  [512128/786991]\n",
            "loss: 0.517504  [524928/786991]\n",
            "loss: 0.493035  [537728/786991]\n",
            "loss: 0.523385  [550528/786991]\n",
            "loss: 0.486832  [563328/786991]\n",
            "loss: 0.518732  [576128/786991]\n",
            "loss: 0.491881  [588928/786991]\n",
            "loss: 0.539362  [601728/786991]\n",
            "loss: 0.539332  [614528/786991]\n",
            "loss: 0.490394  [627328/786991]\n",
            "loss: 0.503770  [640128/786991]\n",
            "loss: 0.559107  [652928/786991]\n",
            "loss: 0.524802  [665728/786991]\n",
            "loss: 0.484257  [678528/786991]\n",
            "loss: 0.539800  [691328/786991]\n",
            "loss: 0.499123  [704128/786991]\n",
            "loss: 0.497758  [716928/786991]\n",
            "loss: 0.533174  [729728/786991]\n",
            "loss: 0.524621  [742528/786991]\n",
            "loss: 0.511205  [755328/786991]\n",
            "loss: 0.528876  [768128/786991]\n",
            "loss: 0.522915  [780928/786991]\n",
            "Performance: \n",
            " R^2: 0.497339, RMSE: 0.707639 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.544536  [  128/786991]\n",
            "loss: 0.487253  [12928/786991]\n",
            "loss: 0.514132  [25728/786991]\n",
            "loss: 0.497079  [38528/786991]\n",
            "loss: 0.481971  [51328/786991]\n",
            "loss: 0.521344  [64128/786991]\n",
            "loss: 0.481591  [76928/786991]\n",
            "loss: 0.501912  [89728/786991]\n",
            "loss: 0.492887  [102528/786991]\n",
            "loss: 0.525663  [115328/786991]\n",
            "loss: 0.538664  [128128/786991]\n",
            "loss: 0.511211  [140928/786991]\n",
            "loss: 0.546072  [153728/786991]\n",
            "loss: 0.469585  [166528/786991]\n",
            "loss: 0.494939  [179328/786991]\n",
            "loss: 0.501975  [192128/786991]\n",
            "loss: 0.499626  [204928/786991]\n",
            "loss: 0.530062  [217728/786991]\n",
            "loss: 0.495853  [230528/786991]\n",
            "loss: 0.524219  [243328/786991]\n",
            "loss: 0.530296  [256128/786991]\n",
            "loss: 0.507738  [268928/786991]\n",
            "loss: 0.518094  [281728/786991]\n",
            "loss: 0.535857  [294528/786991]\n",
            "loss: 0.549632  [307328/786991]\n",
            "loss: 0.529144  [320128/786991]\n",
            "loss: 0.460459  [332928/786991]\n",
            "loss: 0.484648  [345728/786991]\n",
            "loss: 0.526736  [358528/786991]\n",
            "loss: 0.530224  [371328/786991]\n",
            "loss: 0.510698  [384128/786991]\n",
            "loss: 0.498282  [396928/786991]\n",
            "loss: 0.519832  [409728/786991]\n",
            "loss: 0.518099  [422528/786991]\n",
            "loss: 0.495821  [435328/786991]\n",
            "loss: 0.520251  [448128/786991]\n",
            "loss: 0.540893  [460928/786991]\n",
            "loss: 0.512974  [473728/786991]\n",
            "loss: 0.517043  [486528/786991]\n",
            "loss: 0.525856  [499328/786991]\n",
            "loss: 0.534068  [512128/786991]\n",
            "loss: 0.519172  [524928/786991]\n",
            "loss: 0.516899  [537728/786991]\n",
            "loss: 0.494634  [550528/786991]\n",
            "loss: 0.519129  [563328/786991]\n",
            "loss: 0.516977  [576128/786991]\n",
            "loss: 0.500937  [588928/786991]\n",
            "loss: 0.547832  [601728/786991]\n",
            "loss: 0.500376  [614528/786991]\n",
            "loss: 0.506408  [627328/786991]\n",
            "loss: 0.490676  [640128/786991]\n",
            "loss: 0.471317  [652928/786991]\n",
            "loss: 0.524337  [665728/786991]\n",
            "loss: 0.530251  [678528/786991]\n",
            "loss: 0.515024  [691328/786991]\n",
            "loss: 0.526774  [704128/786991]\n",
            "loss: 0.528210  [716928/786991]\n",
            "loss: 0.529833  [729728/786991]\n",
            "loss: 0.516175  [742528/786991]\n",
            "loss: 0.554262  [755328/786991]\n",
            "loss: 0.482454  [768128/786991]\n",
            "loss: 0.512459  [780928/786991]\n",
            "Performance: \n",
            " R^2: 0.497917, RMSE: 0.707252 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.491898  [  128/786991]\n",
            "loss: 0.505389  [12928/786991]\n",
            "loss: 0.459625  [25728/786991]\n",
            "loss: 0.468367  [38528/786991]\n",
            "loss: 0.483331  [51328/786991]\n",
            "loss: 0.486526  [64128/786991]\n",
            "loss: 0.557994  [76928/786991]\n",
            "loss: 0.525565  [89728/786991]\n",
            "loss: 0.535426  [102528/786991]\n",
            "loss: 0.519061  [115328/786991]\n",
            "loss: 0.536771  [128128/786991]\n",
            "loss: 0.497391  [140928/786991]\n",
            "loss: 0.556042  [153728/786991]\n",
            "loss: 0.529807  [166528/786991]\n",
            "loss: 0.513901  [179328/786991]\n",
            "loss: 0.532348  [192128/786991]\n",
            "loss: 0.515171  [204928/786991]\n",
            "loss: 0.516357  [217728/786991]\n",
            "loss: 0.540140  [230528/786991]\n",
            "loss: 0.515158  [243328/786991]\n",
            "loss: 0.490826  [256128/786991]\n",
            "loss: 0.492049  [268928/786991]\n",
            "loss: 0.553746  [281728/786991]\n",
            "loss: 0.541684  [294528/786991]\n",
            "loss: 0.518510  [307328/786991]\n",
            "loss: 0.518761  [320128/786991]\n",
            "loss: 0.522122  [332928/786991]\n",
            "loss: 0.513763  [345728/786991]\n",
            "loss: 0.500519  [358528/786991]\n",
            "loss: 0.536800  [371328/786991]\n",
            "loss: 0.524832  [384128/786991]\n",
            "loss: 0.475757  [396928/786991]\n",
            "loss: 0.513490  [409728/786991]\n",
            "loss: 0.539264  [422528/786991]\n",
            "loss: 0.534244  [435328/786991]\n",
            "loss: 0.522292  [448128/786991]\n",
            "loss: 0.516707  [460928/786991]\n",
            "loss: 0.484940  [473728/786991]\n",
            "loss: 0.483534  [486528/786991]\n",
            "loss: 0.509377  [499328/786991]\n",
            "loss: 0.525744  [512128/786991]\n",
            "loss: 0.477391  [524928/786991]\n",
            "loss: 0.493315  [537728/786991]\n",
            "loss: 0.486917  [550528/786991]\n",
            "loss: 0.517874  [563328/786991]\n",
            "loss: 0.498051  [576128/786991]\n",
            "loss: 0.506699  [588928/786991]\n",
            "loss: 0.493630  [601728/786991]\n",
            "loss: 0.491969  [614528/786991]\n",
            "loss: 0.530247  [627328/786991]\n",
            "loss: 0.477532  [640128/786991]\n",
            "loss: 0.484117  [652928/786991]\n",
            "loss: 0.505761  [665728/786991]\n",
            "loss: 0.507976  [678528/786991]\n",
            "loss: 0.519402  [691328/786991]\n",
            "loss: 0.518921  [704128/786991]\n",
            "loss: 0.533733  [716928/786991]\n",
            "loss: 0.529015  [729728/786991]\n",
            "loss: 0.472837  [742528/786991]\n",
            "loss: 0.500372  [755328/786991]\n",
            "loss: 0.510997  [768128/786991]\n",
            "loss: 0.479158  [780928/786991]\n",
            "Performance: \n",
            " R^2: 0.492981, RMSE: 0.710725 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is finished training, we can test it. The performance metrics indicated above reflect performance in the validation set, but that could be inflated. Using the completely held-out test set should give us an unbiased assay of autoencoder performance."
      ],
      "metadata": {
        "id": "WlV-ghLHp_EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_nn(test_dataloader, ae, loss);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FYx9kTupcBb",
        "outputId": "00439910-b6d5-4876-bbc0-9795b2b383a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance: \n",
            " R^2: 0.492630, RMSE: 0.710772 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the autoencoder does outperform the PCA solution, explaining about 3%-points more of the variance in individual item responses. The differences in performance between the PCA and autoencoder are small, but similar in magnitude to the difference between the PCA and survey scoring methods. This suggests that the assumption of nonlinearity is wrong to about the same degree as the assumption of equal item weighting in this case."
      ],
      "metadata": {
        "id": "LPO5rAlyxNN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing autoencoder representations to scoring\n",
        "\n",
        "In order to use the autoencoder to encode data in its compressed 5d bottleneck space, we cut the network in half - removing the decoder so that the bottleneck layer becomes the output layer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d6xUaMVCqOdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create encoder-only class to extract bottleneck representations\n",
        "class encoder(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.features = nn.Sequential(\n",
        "                    *list(ae.layer_stack.children())[:-16]\n",
        "                )\n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                return x\n",
        "eonly = encoder()\n",
        "\n",
        "# create representation extraction function\n",
        "def ae_code(x,model):\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    return pred.cpu().detach().numpy()\n",
        "\n",
        "# extract test set representation\n",
        "aec = ae_code(test_tensor,eonly)"
      ],
      "metadata": {
        "id": "OQsoMmXFlHJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate out of sample correlation between rotated embedding and Big 5 scores\n",
        "\n",
        "# preparation\n",
        "aec = pd.DataFrame(aec)\n",
        "zaec = aec.apply(zscore)\n",
        "ztestScored = testScored.apply(zscore)\n",
        "\n",
        "rmats = np.zeros((5,5))\n",
        "n = aec.shape[0]\n",
        "\n",
        "# learn procrutes rotation on one half of data\n",
        "sel = np.random.choice(n,int(np.round(n/2)),replace=False)\n",
        "R = orthogonal_procrustes(zaec.iloc[sel,],ztestScored.iloc[sel,])\n",
        "\n",
        "# rotate other half of data\n",
        "nsel = [x for x in range(n) if x not in sel]\n",
        "zraec = np.dot(zaec.iloc[nsel,],R[0])\n",
        "\n",
        "# populate correlation matrix\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        rmats[i,j] = np.corrcoef(ztestScored.iloc[nsel,i],zraec[:,j])[0,1]"
      ],
      "metadata": {
        "id": "DAqPetiNmZSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot results\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.set(font_scale=1.4)\n",
        "tlabs = ['extraversion', 'stability', 'agreeableness', 'conscientiousness', 'openness']\n",
        "nlabs = ['code' + str(i+1) for i in range(5)]\n",
        "sns.heatmap(rmats,xticklabels=nlabs,yticklabels=tlabs,vmin=0.,vmax=1.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "qKBE0M7m0Dct",
        "outputId": "a96ff2b5-53fc-4b58-e616-55cf50e7cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "As we've seen, an autoencoder can learn a representation of personality very similar to that derived via PCA, as well as by the intended scoring of the IPIP Big 5 survey. The autoencoder performs slightly better at reconstructing individual item responses out of sample, but the difference is relatively small. This small difference likely reflects the fact that personality survey questions are selected for their good *linear* response properties. Items with nonlinear response properties would likely be filtered out during the scale construction process. As such, there is little benefit to using a model that can capitalize on nonlinear structure, since this structure has been excluded by design. That said, it's possible that one could construct more efficient (i.e., shorter) scales using questions with nonlinear response properties, provided that one used a model like an autoencoder, rather than a linear model like PCA or summation of ratings, to score people's responses.\n",
        "\n",
        "If you're interested in learning more about autoencoders, the two of the most popular extensions of this method are:\n",
        "* De-noising autoencoders: rather than feeding the model the exact same data as both input and output, the training inputs to a de-noising autoencoder consist of outputs with noise artificially added to them. Since the desired outputs don't have this noise, the model learns to remove the noise from the inputs (in addition to compressing them). These models can then be used to de-noise genuinely noise data, provided that the noise in the real data has the same statistical properties as the artificial noise in the training data.\n",
        "* Variational autoencoders: variational autoencoders (VAEs) feature probabilistic hidden units in their bottleneck layer. This means that each of these units represents a distribution, rather than a single value. These probabilistic units allow the VAEs to become generative models, capable of producing novel outputs by interpolating within their latent bottleneck representations."
      ],
      "metadata": {
        "id": "oKLCCDZzrTWp"
      }
    }
  ]
}